{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "import pandas as pd\n",
    "import tables\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('/home/iprovilkov/data/JUNO/notebooks/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils.data_processing import get_data_2dprojection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 2473904512027844131, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 7555216180\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 11139965184008143280\n",
       " physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 28a6:00:00.0, compute capability: 3.7\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "import keras.layers as L\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "\n",
    "#\n",
    "# image dimensions\n",
    "#\n",
    "\n",
    "img_height = 150\n",
    "img_width = 150\n",
    "img_channels = 2\n",
    "\n",
    "#\n",
    "# network params\n",
    "#\n",
    "\n",
    "cardinality = 1\n",
    "\n",
    "\n",
    "def residual_network_tail(x):\n",
    "    \"\"\"\n",
    "    ResNeXt by default. For ResNet set `cardinality` = 1 above.\n",
    "    \n",
    "    \"\"\"\n",
    "    def add_common_layers(y):\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        y = layers.LeakyReLU()(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def grouped_convolution(y, nb_channels, _strides):\n",
    "        # when `cardinality` == 1 this is just a standard convolution\n",
    "        if cardinality == 1:\n",
    "            return layers.Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
    "        \n",
    "        assert not nb_channels % cardinality\n",
    "        _d = nb_channels // cardinality\n",
    "\n",
    "        # in a grouped convolution layer, input and output channels are divided into `cardinality` groups,\n",
    "        # and convolutions are separately performed within each group\n",
    "        groups = []\n",
    "        for j in range(cardinality):\n",
    "            group = layers.Lambda(lambda z: z[:, :, :, j * _d:j * _d + _d])(y)\n",
    "            groups.append(layers.Conv2D(_d, kernel_size=(3, 3), strides=_strides, padding='same')(group))\n",
    "            \n",
    "        # the grouped convolutional layer concatenates them as the outputs of the layer\n",
    "        y = layers.concatenate(groups)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def residual_block(y, nb_channels_in, nb_channels_out, _strides=(1, 1), _project_shortcut=False):\n",
    "        \"\"\"\n",
    "        Our network consists of a stack of residual blocks. These blocks have the same topology,\n",
    "        and are subject to two simple rules:\n",
    "        - If producing spatial maps of the same size, the blocks share the same hyper-parameters (width and filter sizes).\n",
    "        - Each time the spatial map is down-sampled by a factor of 2, the width of the blocks is multiplied by a factor of 2.\n",
    "        \"\"\"\n",
    "        shortcut = y\n",
    "\n",
    "        # we modify the residual building block as a bottleneck design to make the network more economical\n",
    "        y = layers.Conv2D(nb_channels_in, kernel_size=(1, 1), strides=(1, 1), padding='same')(y)\n",
    "        y = add_common_layers(y)\n",
    "\n",
    "        # ResNeXt (identical to ResNet when `cardinality` == 1)\n",
    "        y = grouped_convolution(y, nb_channels_in, _strides=_strides)\n",
    "        y = add_common_layers(y)\n",
    "\n",
    "        y = layers.Conv2D(nb_channels_out, kernel_size=(1, 1), strides=(1, 1), padding='same')(y)\n",
    "        # batch normalization is employed after aggregating the transformations and before adding to the shortcut\n",
    "        y = layers.BatchNormalization()(y)\n",
    "\n",
    "        # identity shortcuts used directly when the input and output are of the same dimensions\n",
    "        if _project_shortcut or _strides != (1, 1):\n",
    "            # when the dimensions increase projection shortcut is used to match dimensions (done by 1Ã—1 convolutions)\n",
    "            # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
    "            shortcut = layers.Conv2D(nb_channels_out, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
    "            shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "        y = layers.add([shortcut, y])\n",
    "\n",
    "        # relu is performed right after each batch normalization,\n",
    "        # expect for the output of the block where relu is performed after the adding to the shortcut\n",
    "        y = layers.LeakyReLU()(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    # conv1\n",
    "    x = layers.Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding='same')(x)\n",
    "    x = add_common_layers(x)\n",
    "\n",
    "    # conv2\n",
    "    x = layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "    for i in range(3):\n",
    "        project_shortcut = True if i == 0 else False\n",
    "        x = residual_block(x, 128, 256, _project_shortcut=project_shortcut)\n",
    "\n",
    "    # conv3\n",
    "    for i in range(4):\n",
    "        # down-sampling is performed by conv3_1, conv4_1, and conv5_1 with a stride of 2\n",
    "        strides = (2, 2) if i == 0 else (1, 1)\n",
    "        x = residual_block(x, 256, 512, _strides=strides)\n",
    "\n",
    "    # conv4\n",
    "    for i in range(6):\n",
    "        strides = (2, 2) if i == 0 else (1, 1)\n",
    "        x = residual_block(x, 512, 1024, _strides=strides)\n",
    "\n",
    "    # conv5\n",
    "    #for i in range(3):\n",
    "    #    strides = (2, 2) if i == 0 else (1, 1)\n",
    "    #    x = residual_block(x, 1024, 2048, _strides=strides)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LTRAIN = '/mnt/iprovilkov/data_dir/npdata/'\n",
    "rg = np.arange(0,100000,20000)\n",
    "MAXR=17200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_DIR = '/storage/home/vprov/JUNO/JUNO_students/data/'\n",
    "#TRAIN_DIR = '/srv/hd5/data/vprov/data/phase_1/train/'\n",
    "#TRAIN_DIR = '~/data/data_dir/'\n",
    "#tr_lpmt_hits = pd.read_hdf(TRAIN_DIR + 'lpmt_hits.h5', mode='r')\n",
    "#tr_spmt_hits = pd.read_hdf(TRAIN_DIR + 'spmt_hits.h5', mode='r')\n",
    "#tr_pos = pd.read_csv(TRAIN_DIR + 'lpmt_pos.csv') \n",
    "#tr_spmt_pos = pd.read_csv(TRAIN_DIR + 'spmt_pos.csv') \n",
    "#tr_true_info = pd.read_csv(TRAIN_DIR + 'true_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyproj import Proj, transform\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "def lat(x,y,z):\n",
    "    return np.arcsin(z/np.sqrt(x**2 + y**2 + z**2))\n",
    "\n",
    "def lon(x,y,z):\n",
    "    return np.arctan2(y,x)\n",
    "\n",
    "\n",
    "def get_data_2dprojection(lpmt_hits, spmt_hits, pos, true_info, edge_size0=226, edge_size1=111, use_spmt=False, time=None, proj='sin'):\n",
    "    \"\"\"\n",
    "    Transfer data into 2d projection (moll) with 1 channel\n",
    "    Params:\n",
    "    ..., \n",
    "    edge_size : int - projection image size, \n",
    "    use_spmt : bool\n",
    "    \n",
    "    Returns:\n",
    "    data_lpmt: pd.DataFrame\n",
    "    event_to_id: dict\n",
    "    \"\"\"\n",
    "    channels = 1\n",
    "    if time == 'min':\n",
    "        channels = 2\n",
    "    \n",
    "    latt  = np.array(list(map(lambda el: lat(el[0],el[1],el[2]), zip(pos['pmt_x'], pos['pmt_y'], pos['pmt_z']))))\n",
    "    lontt = np.array(list(map(lambda el: lon(el[0],el[1],el[2]), zip(pos['pmt_x'], pos['pmt_y'], pos['pmt_z']))))\n",
    "    \n",
    "    if proj == 'sin':\n",
    "        proj0min = -np.pi\n",
    "        proj0max = np.pi\n",
    "        proj1min = -np.pi/2\n",
    "        proj1max = np.pi/2\n",
    "\n",
    "        proj0 = lontt * np.cos(latt)\n",
    "        proj1 = latt\n",
    "    \n",
    "    pos['proj0'] = np.round((proj0 - proj0min) / (proj0max - proj0min) * (edge_size0-1)).astype(int)\n",
    "    pos['proj1'] = np.round((proj1 - proj1min) / (proj1max - proj1min) * (edge_size1-1)).astype(int)\n",
    "        \n",
    "    merged_hits = pd.merge(lpmt_hits, pos, left_on='pmtID', right_on='pmt_id')\n",
    "   \n",
    "    n = len(lpmt_hits['event'].unique())\n",
    "    data_lpmt = np.zeros((n, edge_size0, edge_size1, channels), dtype='float32')\n",
    "    \n",
    "    event_to_id = {x:y for y, x in enumerate(sorted(merged_hits['event'].unique()))}\n",
    "    \n",
    "    print(\"Starting cycle...\")\n",
    "    if time is None:\n",
    "        for event, mol0i, mol1i in tqdm_notebook(zip(merged_hits['event'], merged_hits['proj0'], merged_hits['proj1'])):\n",
    "            data_lpmt[event_to_id[event]][mol0i, mol1i] += 1\n",
    "    elif time == 'min':\n",
    "        EPS = 1e-7\n",
    "        data_lpmt[:,:,:,1] = -EPS\n",
    "        # Calculate min time for each event\n",
    "        ev_min = merged_hits[['event', 'hitTime']].groupby('event').min()\n",
    "        event2min = {id_: min_ for id_, min_ in zip(ev_min.index, ev_min.hitTime)}\n",
    "        \n",
    "        for event, mol0i, mol1i, time in tqdm_notebook(zip(merged_hits['event'], merged_hits['proj0'], merged_hits['proj1'], merged_hits['hitTime'])):\n",
    "            event_id = event_to_id[event]\n",
    "            data_lpmt[event_id][mol0i, mol1i][0] += 1\n",
    "            event_min = event2min[event]\n",
    "            cur_min = data_lpmt[event_id][mol0i, mol1i][1]\n",
    "            if cur_min < 0 or cur_min > (time - event_min):\n",
    "                data_lpmt[event_id][mol0i, mol1i][1] = time - event_min\n",
    "    \"\"\"\n",
    "    elif time == 'minmax':\n",
    "        # Add max and min for each cell\n",
    "        for event, mol0i, mol1i, time in tqdm_notebook(zip(merged_hits['event'], merged_hits['mol0i'], merged_hits['mol1i'], merged_hits['hitTime'])):\n",
    "            data_lpmt[event_to_id[event]][mol0i, mol1i][0] += 1\n",
    "            cur_min = data_lpmt[event_to_id[event]][mol0i, mol1i][1]\n",
    "            cur_max = data_lpmt[event_to_id[event]][mol0i, mol1i][2]\n",
    "            if cur_min == 0:\n",
    "                data_lpmt[event_to_id[event]][mol0i, mol1i][1] = time\n",
    "            if cur_max == 0:\n",
    "                data_lpmt[event_to_id[event]][mol0i, mol1i][2] = time\n",
    "            if cur_min > time:\n",
    "                data_lpmt[event_to_id[event]][mol0i, mol1i][1] = time\n",
    "            if cur_max < time:\n",
    "                data_lpmt[event_to_id[event]][mol0i, mol1i][2] = time\n",
    "    \"\"\"\n",
    "    \n",
    "    return data_lpmt, event_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cycle...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b72c6918d824b2c8832b53347aa07c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting cycle...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59b77b70b784f70b06db21520e360ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for start, end  in zip(rg[:-1], rg[1:]):\n",
    "    data_lpmt, event_to_id = get_data_2dprojection(tr_lpmt_hits[(tr_lpmt_hits['event'] >= start) & (tr_lpmt_hits['event'] < end)\n",
    "                                                                & (tr_lpmt_hits['isDN'] == False)], \n",
    "                                                None, \n",
    "                                                tr_pos, \n",
    "                                                tr_true_info[(tr_true_info.R<=MAXR) \n",
    "                                                             & (tr_true_info['evtID'] >= start) \n",
    "                                                             & (tr_true_info['evtID'] < end)],\n",
    "                                                edge_size0=226,\n",
    "                                                edge_size1=112,\n",
    "                                                use_spmt=False,\n",
    "                                                time='min',\n",
    "                                                )\n",
    "    np.save(LTRAIN + '_' + str(start) + 'without_noise' + '_sin', data_lpmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling2D, Dense\n",
    "from keras.models import Model\n",
    "import keras.layers as L\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.engine.input_layer import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_size0 = 226\n",
    "edge_size1 = 112\n",
    "inputs = Input(shape=(edge_size0,edge_size1,2))\n",
    "base_model = residual_network_tail(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = L.MaxPool2D()(base_model)\n",
    "x = L.Flatten()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = L.Dropout(0.5)(x)\n",
    "x = Dense(1024, activation='elu')(x)\n",
    "x = L.Dropout(0.5)(x)\n",
    "x = Dense(512, activation='elu')(x)\n",
    "predictions = Dense(1)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[70].trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(decay=1e-4), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 226, 112, 2)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 113, 56, 64)  6336        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 113, 56, 64)  256         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)      (None, 113, 56, 64)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 57, 28, 64)   0           leaky_re_lu_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 57, 28, 128)  8320        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 57, 28, 128)  512         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_42 (LeakyReLU)      (None, 57, 28, 128)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 57, 28, 128)  147584      leaky_re_lu_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 57, 28, 128)  512         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)      (None, 57, 28, 128)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 57, 28, 256)  16640       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 57, 28, 256)  33024       leaky_re_lu_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 57, 28, 256)  1024        conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 57, 28, 256)  1024        conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 57, 28, 256)  0           batch_normalization_48[0][0]     \n",
      "                                                                 batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_44 (LeakyReLU)      (None, 57, 28, 256)  0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 57, 28, 128)  32896       leaky_re_lu_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 57, 28, 128)  512         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_45 (LeakyReLU)      (None, 57, 28, 128)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 57, 28, 128)  147584      leaky_re_lu_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 57, 28, 128)  512         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_46 (LeakyReLU)      (None, 57, 28, 128)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 57, 28, 256)  33024       leaky_re_lu_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 57, 28, 256)  1024        conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 57, 28, 256)  0           leaky_re_lu_44[0][0]             \n",
      "                                                                 batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_47 (LeakyReLU)      (None, 57, 28, 256)  0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 57, 28, 128)  32896       leaky_re_lu_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 57, 28, 128)  512         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_48 (LeakyReLU)      (None, 57, 28, 128)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 57, 28, 128)  147584      leaky_re_lu_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 57, 28, 128)  512         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_49 (LeakyReLU)      (None, 57, 28, 128)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 57, 28, 256)  33024       leaky_re_lu_49[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 57, 28, 256)  1024        conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 57, 28, 256)  0           leaky_re_lu_47[0][0]             \n",
      "                                                                 batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_50 (LeakyReLU)      (None, 57, 28, 256)  0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 57, 28, 256)  65792       leaky_re_lu_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 57, 28, 256)  1024        conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_51 (LeakyReLU)      (None, 57, 28, 256)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 29, 14, 256)  590080      leaky_re_lu_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 29, 14, 256)  1024        conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_52 (LeakyReLU)      (None, 29, 14, 256)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 29, 14, 512)  131584      leaky_re_lu_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 29, 14, 512)  131584      leaky_re_lu_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 29, 14, 512)  2048        conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 29, 14, 512)  2048        conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 29, 14, 512)  0           batch_normalization_58[0][0]     \n",
      "                                                                 batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_53 (LeakyReLU)      (None, 29, 14, 512)  0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 29, 14, 256)  131328      leaky_re_lu_53[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 29, 14, 256)  1024        conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_54 (LeakyReLU)      (None, 29, 14, 256)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 29, 14, 256)  590080      leaky_re_lu_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 29, 14, 256)  1024        conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_55 (LeakyReLU)      (None, 29, 14, 256)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 29, 14, 512)  131584      leaky_re_lu_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 29, 14, 512)  2048        conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 29, 14, 512)  0           leaky_re_lu_53[0][0]             \n",
      "                                                                 batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_56 (LeakyReLU)      (None, 29, 14, 512)  0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 29, 14, 256)  131328      leaky_re_lu_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 29, 14, 256)  1024        conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_57 (LeakyReLU)      (None, 29, 14, 256)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 29, 14, 256)  590080      leaky_re_lu_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 29, 14, 256)  1024        conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_58 (LeakyReLU)      (None, 29, 14, 256)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 29, 14, 512)  131584      leaky_re_lu_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 29, 14, 512)  2048        conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 29, 14, 512)  0           leaky_re_lu_56[0][0]             \n",
      "                                                                 batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_59 (LeakyReLU)      (None, 29, 14, 512)  0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 29, 14, 256)  131328      leaky_re_lu_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 29, 14, 256)  1024        conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_60 (LeakyReLU)      (None, 29, 14, 256)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 29, 14, 256)  590080      leaky_re_lu_60[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 29, 14, 256)  1024        conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_61 (LeakyReLU)      (None, 29, 14, 256)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 29, 14, 512)  131584      leaky_re_lu_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 29, 14, 512)  2048        conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 29, 14, 512)  0           leaky_re_lu_59[0][0]             \n",
      "                                                                 batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_62 (LeakyReLU)      (None, 29, 14, 512)  0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 29, 14, 512)  262656      leaky_re_lu_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 29, 14, 512)  2048        conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_63 (LeakyReLU)      (None, 29, 14, 512)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 15, 7, 512)   2359808     leaky_re_lu_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 15, 7, 512)   2048        conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_64 (LeakyReLU)      (None, 15, 7, 512)   0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 15, 7, 1024)  525312      leaky_re_lu_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 15, 7, 1024)  525312      leaky_re_lu_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 15, 7, 1024)  4096        conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 15, 7, 1024)  4096        conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 15, 7, 1024)  0           batch_normalization_71[0][0]     \n",
      "                                                                 batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_65 (LeakyReLU)      (None, 15, 7, 1024)  0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 15, 7, 512)   524800      leaky_re_lu_65[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 15, 7, 512)   2048        conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_66 (LeakyReLU)      (None, 15, 7, 512)   0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 15, 7, 512)   2359808     leaky_re_lu_66[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 15, 7, 512)   2048        conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_67 (LeakyReLU)      (None, 15, 7, 512)   0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 15, 7, 1024)  525312      leaky_re_lu_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 15, 7, 1024)  4096        conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 15, 7, 1024)  0           leaky_re_lu_65[0][0]             \n",
      "                                                                 batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_68 (LeakyReLU)      (None, 15, 7, 1024)  0           add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 15, 7, 512)   524800      leaky_re_lu_68[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 15, 7, 512)   2048        conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_69 (LeakyReLU)      (None, 15, 7, 512)   0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 15, 7, 512)   2359808     leaky_re_lu_69[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 15, 7, 512)   2048        conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_70 (LeakyReLU)      (None, 15, 7, 512)   0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 15, 7, 1024)  525312      leaky_re_lu_70[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 15, 7, 1024)  4096        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 15, 7, 1024)  0           leaky_re_lu_68[0][0]             \n",
      "                                                                 batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_71 (LeakyReLU)      (None, 15, 7, 1024)  0           add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 15, 7, 512)   524800      leaky_re_lu_71[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 15, 7, 512)   2048        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_72 (LeakyReLU)      (None, 15, 7, 512)   0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 15, 7, 512)   2359808     leaky_re_lu_72[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 15, 7, 512)   2048        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_73 (LeakyReLU)      (None, 15, 7, 512)   0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 15, 7, 1024)  525312      leaky_re_lu_73[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 15, 7, 1024)  4096        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 15, 7, 1024)  0           leaky_re_lu_71[0][0]             \n",
      "                                                                 batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_74 (LeakyReLU)      (None, 15, 7, 1024)  0           add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 15, 7, 512)   524800      leaky_re_lu_74[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 15, 7, 512)   2048        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_75 (LeakyReLU)      (None, 15, 7, 512)   0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 15, 7, 512)   2359808     leaky_re_lu_75[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 15, 7, 512)   2048        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_76 (LeakyReLU)      (None, 15, 7, 512)   0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 15, 7, 1024)  525312      leaky_re_lu_76[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 15, 7, 1024)  4096        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 15, 7, 1024)  0           leaky_re_lu_74[0][0]             \n",
      "                                                                 batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_77 (LeakyReLU)      (None, 15, 7, 1024)  0           add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 15, 7, 512)   524800      leaky_re_lu_77[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 15, 7, 512)   2048        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_78 (LeakyReLU)      (None, 15, 7, 512)   0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 15, 7, 512)   2359808     leaky_re_lu_78[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 15, 7, 512)   2048        conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_79 (LeakyReLU)      (None, 15, 7, 512)   0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 15, 7, 1024)  525312      leaky_re_lu_79[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 15, 7, 1024)  4096        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 15, 7, 1024)  0           leaky_re_lu_77[0][0]             \n",
      "                                                                 batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_80 (LeakyReLU)      (None, 15, 7, 1024)  0           add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 7, 3, 1024)   0           leaky_re_lu_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 21504)        0           max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 21504)        0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1024)         22021120    dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1024)         0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 512)          524800      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            513         dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 47,465,153\n",
      "Trainable params: 47,425,601\n",
      "Non-trainable params: 39,552\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build(input_shape=(None, edge_size0,edge_size1,2))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = 'ResNet_sinus_R.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "csv_logger = CSVLogger(logdir, append=True, separator=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = '~/data/data_dir/'\n",
    "tr_true_info = pd.read_csv(TRAIN_DIR + 'true_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epochs = [5,5,5,3,3,3,1,1,1,1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load(LTRAIN + '_' + str(rg[-2]) +'without_noise_sin.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = tr_true_info[(tr_true_info['evtID'] >= rg[-2]) \n",
    "                     & (tr_true_info['evtID'] < rg[-1])]\n",
    "mask = (y_test.R <= MAXR)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask][['R']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X):\n",
    "    X[:,:,:,1] /= 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_R(y):\n",
    "    y /= 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler(X,y):\n",
    "    X[:,:,:,1] /= 20 \n",
    "    y /= 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_R(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gc\n",
    "#for i in range(len(epochs)):\n",
    "#    for start, end in zip(rg[:-2], rg[1:-1]):\n",
    "#        print(\"ITERATION: \", i)\n",
    "#        data_lpmt_cur = np.load(LTRAIN + '_' + str(start) +'without_noise_sin.npy')\n",
    "#        ys = tr_true_info[(tr_true_info['evtID'] >= start) \n",
    "#                            & (tr_true_info['evtID'] < end)]\n",
    "#        mask = (ys.R <= MAXR)\n",
    "#        data_lpmt_cur = data_lpmt_cur[mask]\n",
    "#        y_cur = ys[mask][['E']].values\n",
    "#        #Scale\n",
    "#        scale(data_lpmt_cur)\n",
    "#        scale_R(y_cur)\n",
    "#        #X_train, X_test, y_train, y_test = train_test_split(data_lpmt, y_val, test_size=0.2, random_state=45)\n",
    "#        out = model.fit(data_lpmt_cur, y_cur, batch_size=24, \n",
    "#                epochs=epochs[i],\n",
    "#                callbacks=[csv_logger, reduce_lr],\n",
    "#                validation_data=(X_test, y_test),\n",
    "#                shuffle=True)\n",
    "#    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, batch_size, rg, file_names, y_true, scaler, \n",
    "                 MAXR=17200, target_names=['E'],shuffle=True):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.rg = rg\n",
    "        self.shuffle = shuffle\n",
    "        self.counter = 0\n",
    "        self.file_names=file_names\n",
    "        self.y_true = y_true\n",
    "        self.scaler = scaler\n",
    "        self.target_names = target_names\n",
    "        self.MAXR=MAXR\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.indexes) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        X = self.data_lpmt[indexes]\n",
    "        y = self.y_cur[indexes]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        i = self.counter % len(self.file_names)\n",
    "        self.data_lpmt = np.load(self.file_names[i])\n",
    "        start = self.rg[i]\n",
    "        end = self.rg[i+1]\n",
    "        ys = self.y_true[(self.y_true['evtID'] >= start) \n",
    "                            & (self.y_true['evtID'] < end)]\n",
    "        mask = (ys.R <= self.MAXR)\n",
    "        self.data_lpmt = self.data_lpmt[mask]\n",
    "        self.y_cur = ys[mask][self.target_names].values\n",
    "        #Scaler\n",
    "        self.scaler(self.data_lpmt, self.y_cur)\n",
    "        \n",
    "        self.indexes = np.arange(self.data_lpmt.shape[0])\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "        self.counter += 1\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(24,rg=rg,\n",
    "                                   file_names=[LTRAIN + '_' + str(start) +'without_noise_sin.npy' for start in rg[:-2]],\n",
    "                                  y_true=tr_true_info,\n",
    "                                  target_names=['R'],\n",
    "                                  scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "764/765 [============================>.] - ETA: 0s - loss: 0.8420\n",
      "765/765 [==============================] - 866s 1s/step - loss: 0.8411 - val_loss: 1.7058\n",
      "Epoch 2/40\n",
      "765/765 [==============================] - 863s 1s/step - loss: 0.3273 - val_loss: 0.0565\n",
      "Epoch 3/40\n",
      "765/765 [==============================] - 863s 1s/step - loss: 0.1274 - val_loss: 0.0240\n",
      "Epoch 4/40\n",
      "765/765 [==============================] - 860s 1s/step - loss: 0.0898 - val_loss: 0.0590\n",
      "Epoch 5/40\n",
      "765/765 [==============================] - 858s 1s/step - loss: 0.1013 - val_loss: 0.0535\n",
      "Epoch 6/40\n",
      "765/765 [==============================] - 857s 1s/step - loss: 0.0600 - val_loss: 0.0227\n",
      "Epoch 7/40\n",
      "765/765 [==============================] - 857s 1s/step - loss: 0.0546 - val_loss: 0.0443\n",
      "Epoch 8/40\n",
      "765/765 [==============================] - 857s 1s/step - loss: 0.0889 - val_loss: 1.7201\n",
      "Epoch 9/40\n",
      "765/765 [==============================] - 844s 1s/step - loss: 0.1969 - val_loss: 0.0243\n",
      "Epoch 10/40\n",
      "765/765 [==============================] - 846s 1s/step - loss: 0.0446 - val_loss: 0.0437\n",
      "Epoch 11/40\n",
      "765/765 [==============================] - 847s 1s/step - loss: 0.0261 - val_loss: 5.6613\n",
      "Epoch 12/40\n",
      "765/765 [==============================] - 849s 1s/step - loss: 0.0145 - val_loss: 0.4107\n",
      "Epoch 13/40\n",
      "765/765 [==============================] - 848s 1s/step - loss: 0.0125 - val_loss: 2.7720\n",
      "Epoch 14/40\n",
      "765/765 [==============================] - 847s 1s/step - loss: 0.0120 - val_loss: 4.1721\n",
      "Epoch 15/40\n",
      "765/765 [==============================] - 848s 1s/step - loss: 0.0102 - val_loss: 6.6489\n",
      "Epoch 16/40\n",
      "765/765 [==============================] - 844s 1s/step - loss: 0.0104 - val_loss: 0.9865\n",
      "Epoch 17/40\n",
      "765/765 [==============================] - 847s 1s/step - loss: 0.0074 - val_loss: 1.5769\n",
      "Epoch 18/40\n",
      "765/765 [==============================] - 851s 1s/step - loss: 0.0066 - val_loss: 1.2132\n",
      "Epoch 19/40\n",
      " 65/765 [=>............................] - ETA: 10:16 - loss: 0.0065"
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator=training_generator,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=6,\n",
    "                    callbacks=[csv_logger, reduce_lr],\n",
    "                   epochs=4*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7febd2640e80>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cs = pd.read_csv(logdir, sep=';')\n",
    "plt.scatter(np.arange(0, cs.shape[0]), cs['val_loss'].values)\n",
    "#plt.ylim(0,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(100 - 10 - 10) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
